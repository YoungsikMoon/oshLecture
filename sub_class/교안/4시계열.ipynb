{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rxIAYBqVP8u"
      },
      "source": [
        "# 1. RNN KOSPI 주가예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MJh6sMeU2of"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cDCdsGMVCOF"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/kospi.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TguCzRcTVCq5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic0Ao-ccVDar"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df[['Open','High','Low','Close','Volume']] = scaler.fit_transform(df[['Open','High','Low','Close','Volume']])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuSPcg3sVEGg"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQIK7-YSVEyW"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'{device} is available')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hFCpRtEVFdQ"
      },
      "outputs": [],
      "source": [
        "X = df[['Open','High','Low','Volume']].values\n",
        "y = df['Close'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7WrZandVGVA"
      },
      "outputs": [],
      "source": [
        "def seq_data(x, y, sequence_length):\n",
        "\n",
        "  x_seq = []\n",
        "  y_seq = []\n",
        "  for i in range(len(x) - sequence_length):\n",
        "    x_seq.append(x[i: i+sequence_length])\n",
        "    y_seq.append(y[i+sequence_length])\n",
        "\n",
        "  return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(y_seq).to(device).view([-1, 1]) # float형 tensor로 변형, gpu사용가능하게 .to(device)를 사용.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-_29pMlVHYy"
      },
      "outputs": [],
      "source": [
        "split = 200\n",
        "sequence_length = 5 # 5일치를 넣는다\n",
        "\n",
        "x_seq, y_seq = seq_data(X, y, sequence_length)\n",
        "\n",
        "x_train_seq = x_seq[:split]\n",
        "y_train_seq = y_seq[:split]\n",
        "x_test_seq = x_seq[split:]\n",
        "y_test_seq = y_seq[split:]\n",
        "print(x_train_seq.size(), y_train_seq.size())\n",
        "print(x_test_seq.size(), y_test_seq.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5ex78TfVInC"
      },
      "outputs": [],
      "source": [
        "train = torch.utils.data.TensorDataset(x_train_seq, y_train_seq)\n",
        "test = torch.utils.data.TensorDataset(x_test_seq, y_test_seq)\n",
        "\n",
        "batch_size = 20\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-MY0-XFVJl8"
      },
      "outputs": [],
      "source": [
        "input_size = x_seq.size(2)\n",
        "num_layers = 2\n",
        "hidden_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqMXqjq5VLBQ"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "    self.device = device\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 1),  #y값은 1개니까 1\n",
        "                            nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.\n",
        "    out, _ = self.rnn(x, h0) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\n",
        "    out = out.reshape(out.shape[0], -1) # many to many 전략\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "    \n",
        "model = VanillaRNN(input_size=input_size,\n",
        "                   hidden_size=hidden_size,\n",
        "                   sequence_length=sequence_length,\n",
        "                   num_layers=num_layers,\n",
        "                   device=device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAfcNJfoVL2r"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "lr = 1e-3\n",
        "num_epochs = 200\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Ge8xFzVM9A"
      },
      "outputs": [],
      "source": [
        "loss_graph = [] # 그래프 그릴 목적인 loss.\n",
        "n = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for data in train_loader:\n",
        "    seq, target = data # 배치 데이터.\n",
        "    out = model(seq)   # 모델에 넣고,\n",
        "    loss = criterion(out, target) # output 가지고 loss 구하고,\n",
        "\n",
        "    optimizer.zero_grad() #\n",
        "    loss.backward() # loss가 최소가 되게하는\n",
        "    optimizer.step() # 가중치 업데이트 해주고,\n",
        "    running_loss += loss.item() # 한 배치의 loss 더해주고,\n",
        "\n",
        "  loss_graph.append(running_loss / n) # 한 epoch에 모든 배치들에 대한 평균 loss 리스트에 담고,\n",
        "  if epoch % 100 == 0:\n",
        "    print('[epoch: %d] loss: %.4f'%(epoch, running_loss/n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDeL8cJrVN10"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(loss_graph)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTOZIWWTVPNV"
      },
      "outputs": [],
      "source": [
        "def plotting(train_loader, test_loader, actual):\n",
        "  with torch.no_grad():\n",
        "    train_pred = []\n",
        "    test_pred = []\n",
        "\n",
        "    for data in train_loader:\n",
        "      seq, target = data\n",
        "      out = model(seq)\n",
        "      train_pred += out.cpu().numpy().tolist()\n",
        "\n",
        "    for data in test_loader:\n",
        "      seq, target = data\n",
        "      out = model(seq)\n",
        "      test_pred += out.cpu().numpy().tolist()\n",
        "\n",
        "  total = train_pred + test_pred\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.plot(np.ones(100)*len(train_pred), np.linspace(0,1,100), '--', linewidth=0.6)\n",
        "  plt.plot(actual, '--')\n",
        "  plt.plot(total, 'b', linewidth=0.6)\n",
        "\n",
        "  plt.legend(['train boundary', 'actual', 'prediction'])\n",
        "  plt.show()\n",
        "\n",
        "plotting(train_loader, test_loader, df['Close'][sequence_length:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrqcfAYoVVlZ"
      },
      "source": [
        "# 2. LSTM 삼성전자 주가예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-QnOOFOVXNd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as pdr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd-x2UtqV3or"
      },
      "outputs": [],
      "source": [
        "start = (2000, 1, 1)  # 2020년 01년 01월\n",
        "start = datetime.datetime(*start)\n",
        "end = datetime.date.today()  # 현재\n",
        "\n",
        "# yahoo 에서 삼성 전자 불러오기\n",
        "df = pdr.DataReader('005930.KS', 'yahoo', start, end)\n",
        "df.head(5)\n",
        "df.tail(5)\n",
        "df.Close.plot(grid=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPg4s9YeV5xi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "저도 주식을 잘 모르기 때문에 참고해주시면 좋을 것 같습니다.\n",
        "open 시가\n",
        "high 고가\n",
        "low 저가\n",
        "close 종가\n",
        "volume 거래량\n",
        "Adj Close 주식의 분할, 배당, 배분 등을 고려해 조정한 종가\n",
        "\n",
        "확실한건 거래량(Volume)은 데이터에서 제하는 것이 중요하고,\n",
        "Y 데이터를 Adj Close로 정합니다. (종가로 해도 된다고 생각합니다.)\n",
        "\n",
        "\"\"\"\n",
        "X = df.drop(columns='Volume')\n",
        "y = df.iloc[:, 5:6]\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "937A9dYyV6_1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "학습이 잘되기 위해 데이터 정규화\n",
        "StandardScaler\t각 특징의 평균을 0, 분산을 1이 되도록 변경\n",
        "MinMaxScaler\t최대/최소값이 각각 1, 0이 되도록 변경\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "X_ss = ss.fit_transform(X)\n",
        "y_mm = mm.fit_transform(y)\n",
        "\n",
        "# Train Data\n",
        "X_train = X_ss[:4500, :]\n",
        "X_test = X_ss[4500:, :]\n",
        "\n",
        "# Test Data\n",
        "\"\"\"\n",
        "( 굳이 없어도 된다. 하지만 얼마나 예측데이터와 실제 데이터의 정확도를 확인하기 위해\n",
        "from sklearn.metrics import accuracy_score 를 통해 정확한 값으로 확인할 수 있다. )\n",
        "\"\"\"\n",
        "y_train = y_mm[:4500, :]\n",
        "y_test = y_mm[4500:, :]\n",
        "\n",
        "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Shape\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92Ctc7-AV89Z"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "torch Variable에는 3개의 형태가 있다.\n",
        "data, grad, grad_fn 한 번 구글에 찾아서 공부해보길 바랍니다.\n",
        "\"\"\"\n",
        "X_train_tensors = Variable(torch.Tensor(X_train))\n",
        "X_test_tensors = Variable(torch.Tensor(X_test))\n",
        "\n",
        "y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "y_test_tensors = Variable(torch.Tensor(y_test))\n",
        "\n",
        "X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
        "X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
        "\n",
        "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
        "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl69XJhcV88B"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # device\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sIjJCdvWGc0"
      },
      "outputs": [],
      "source": [
        "class LSTM1(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "    super(LSTM1, self).__init__()\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.seq_length = seq_length #sequence length\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                      num_layers=num_layers, batch_first=True) #lstm\n",
        "    self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
        "    self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
        "    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
        "    # Propagate input through LSTM\n",
        "\n",
        "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "\n",
        "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    out = self.relu(hn)\n",
        "    out = self.fc_1(out) #first Dense\n",
        "    out = self.relu(out) #relu\n",
        "    out = self.fc(out) #Final Output\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvaqGHlVWKP_"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30000 #1000 epochs\n",
        "learning_rate = 0.00001 #0.001 lr\n",
        "\n",
        "input_size = 5 #number of features\n",
        "hidden_size = 2 #number of features in hidden state\n",
        "num_layers = 1 #number of stacked lstm layers\n",
        "\n",
        "num_classes = 1 #number of output classes\n",
        "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).to(device)\n",
        "\n",
        "loss_function = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)  # adam optimizer\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  outputs = lstm1.forward(X_train_tensors_final.to(device)) #forward pass\n",
        "  optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
        "\n",
        "  # obtain the loss function\n",
        "  loss = loss_function(outputs, y_train_tensors.to(device))\n",
        "\n",
        "  loss.backward() #calculates the loss of the loss function\n",
        "\n",
        "  optimizer.step() #improve from loss, i.e backprop\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "df_X_ss = ss.transform(df.drop(columns='Volume'))\n",
        "df_y_mm = mm.transform(df.iloc[:, 5:6])\n",
        "\n",
        "df_X_ss = Variable(torch.Tensor(df_X_ss)) #converting to Tensors\n",
        "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
        "#reshaping the dataset\n",
        "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], 1, df_X_ss.shape[1]))\n",
        "train_predict = lstm1(df_X_ss.to(device))#forward pass\n",
        "data_predict = train_predict.data.detach().cpu().numpy() #numpy conversion\n",
        "dataY_plot = df_y_mm.data.numpy()\n",
        "\n",
        "data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
        "dataY_plot = mm.inverse_transform(dataY_plot)\n",
        "plt.figure(figsize=(10,6)) #plotting\n",
        "plt.axvline(x=4500, c='r', linestyle='--') #size of the training set\n",
        "\n",
        "plt.plot(dataY_plot, label='Actuall Data') #actual plot\n",
        "plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
        "plt.title('Time-Series Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
